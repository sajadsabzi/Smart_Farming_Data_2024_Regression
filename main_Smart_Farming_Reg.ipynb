{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3942f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da03e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 0: Starting initial setup\n",
      "All directories created or verified successfully.\n",
      "Dataset loaded successfully.\n",
      "\n",
      "Phase 1: Starting Exploratory Data Analysis (EDA)\n",
      "Plot saved: target_variable_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved: correlation_heatmap_all_features\n",
      "Exploratory Data Analysis completed.\n",
      "\n",
      "Phase 1.5: Starting Inferential Statistical Analysis\n",
      "Statistical analysis (ANOVA) completed and results saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3914310/132650710.py:176: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(x='label', y=TARGET_VARIABLE, data=df, ax=ax, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved: boxplot_water_usage_efficiency_by_crop\n",
      "\n",
      "Phase 2: Starting data preprocessing\n",
      "Saving train and test sets for reproducibility...\n",
      "Data preprocessing and splitting completed. All necessary artifacts saved.\n",
      "\n",
      "Phase 3: Starting comprehensive model evaluation and comparison\n",
      "--- Evaluating model: Linear Regression ---\n",
      "Predictions for Linear Regression saved to: /home/sajad/Sajad_test/Smart_Farming_Data_2024 _Regression/data_results/predictions_Linear_Regression.csv\n",
      "Plot saved: evaluation_plots_Linear_Regression\n",
      "--- Evaluating model: Ridge ---\n",
      "Predictions for Ridge saved to: /home/sajad/Sajad_test/Smart_Farming_Data_2024 _Regression/data_results/predictions_Ridge.csv\n",
      "Plot saved: evaluation_plots_Ridge\n",
      "--- Evaluating model: Lasso ---\n",
      "Predictions for Lasso saved to: /home/sajad/Sajad_test/Smart_Farming_Data_2024 _Regression/data_results/predictions_Lasso.csv\n",
      "Plot saved: evaluation_plots_Lasso\n",
      "--- Evaluating model: KNN Regressor ---\n",
      "Predictions for KNN Regressor saved to: /home/sajad/Sajad_test/Smart_Farming_Data_2024 _Regression/data_results/predictions_KNN_Regressor.csv\n",
      "Plot saved: evaluation_plots_KNN_Regressor\n",
      "--- Evaluating model: SVR ---\n",
      "Predictions for SVR saved to: /home/sajad/Sajad_test/Smart_Farming_Data_2024 _Regression/data_results/predictions_SVR.csv\n",
      "Plot saved: evaluation_plots_SVR\n",
      "--- Evaluating model: Random Forest Regressor ---\n",
      "Predictions for Random Forest Regressor saved to: /home/sajad/Sajad_test/Smart_Farming_Data_2024 _Regression/data_results/predictions_Random_Forest_Regressor.csv\n",
      "Plot saved: evaluation_plots_Random_Forest_Regressor\n",
      "--- Evaluating model: XGBoost Regressor ---\n",
      "Predictions for XGBoost Regressor saved to: /home/sajad/Sajad_test/Smart_Farming_Data_2024 _Regression/data_results/predictions_XGBoost_Regressor.csv\n",
      "Plot saved: evaluation_plots_XGBoost_Regressor\n",
      "--- Evaluating model: LightGBM Regressor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajad/miniconda3/envs/sajad/lib/python3.11/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for LightGBM Regressor saved to: /home/sajad/Sajad_test/Smart_Farming_Data_2024 _Regression/data_results/predictions_LightGBM_Regressor.csv\n",
      "Plot saved: evaluation_plots_LightGBM_Regressor\n",
      "Plot saved: models_metrics_comparison_bar_chart\n",
      "\n",
      "Best model based on R-squared: Lasso\n",
      "\n",
      "Phase 4: Starting hyperparameter tuning for Lasso\n",
      "Hyperparameter tuning is not defined for this model.\n",
      "Tuned model Lasso saved in the Best_Model_Analysis folder.\n",
      "\n",
      "Phase 5: Starting in-depth analysis of Lasso with SHAP\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece552096fca4be0a315e27111d19f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values and transformed test data saved for reproducibility.\n",
      "Plot saved: shap_summary_bar_plot\n",
      "Plot saved: shap_summary_dot_plot\n",
      "Creating SHAP dependence plots for top features...\n",
      "Plot saved: shap_dependence_plot_cat__label_watermelon\n",
      "Plot saved: shap_dependence_plot_cat__label_rice\n",
      "Plot saved: shap_dependence_plot_cat__label_pomegranate\n",
      "Plot saved: shap_dependence_plot_cat__label_pigeonpeas\n",
      "Plot saved: shap_dependence_plot_cat__label_papaya\n",
      "\n",
      "================================================================================\n",
      "Main script executed successfully! All results and artifacts are saved.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Comprehensive Data Analysis Script for Q1 Paper - Regression Version\n",
    "# Objective: Predict Water Usage Efficiency\n",
    "# Author: AI Assistant (Adapted for Regression)\n",
    "# Version: 5.6 - Wider Subplot Spacing\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 0: Initial Setup, Libraries, and Data Loading\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Phase 0: Starting initial setup\")\n",
    "\n",
    "# --- Import essential libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "import shap\n",
    "from scipy import stats\n",
    "\n",
    "# --- Import sklearn tools ---\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# --- Import regression models ---\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# --- General Settings ---\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 7,\n",
    "    'axes.titlesize': 7,\n",
    "    'axes.labelsize': 7,\n",
    "    'xtick.labelsize': 7,\n",
    "    'ytick.labelsize': 7,\n",
    "    'legend.fontsize': 7,\n",
    "    'figure.titlesize': 7\n",
    "})\n",
    "RANDOM_STATE = 42\n",
    "TARGET_VARIABLE = 'water_usage_efficiency'\n",
    "\n",
    "# --- Define Paths ---\n",
    "BASE_DIR = '/home/sajad/Sajad_test/Smart_Farming_Data_2024 _Regression/'\n",
    "FILE_NAME = 'Crop_recommendationV2.csv'\n",
    "DATA_PATH = os.path.join(BASE_DIR, 'dataset', FILE_NAME)\n",
    "\n",
    "DATA_RESULTS_PATH = os.path.join(BASE_DIR, 'data_results')\n",
    "COMPARE_PATH = os.path.join(BASE_DIR, 'comparison')\n",
    "EDA_PATH = os.path.join(BASE_DIR, 'EDA')\n",
    "MODELS_PATH = os.path.join(BASE_DIR, 'Models')\n",
    "STATS_PATH = os.path.join(BASE_DIR, 'Statistical_Analysis')\n",
    "BEST_MODEL_PATH = os.path.join(BASE_DIR, 'Best_Model_Analysis')\n",
    "\n",
    "paths_to_create = [DATA_RESULTS_PATH, COMPARE_PATH, EDA_PATH, MODELS_PATH, STATS_PATH, BEST_MODEL_PATH]\n",
    "for path in paths_to_create:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "print(\"All directories created or verified successfully.\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_plot(fig, path, filename, width_cm=15):\n",
    "    \"\"\"Helper function to save plots in various formats.\"\"\"\n",
    "    width_in = width_cm / 2.54\n",
    "    aspect_ratio = fig.get_figheight() / fig.get_figwidth()\n",
    "    fig.set_size_inches(width_in, width_in * aspect_ratio)\n",
    "    fig.savefig(os.path.join(path, f\"{filename}.svg\"), format='svg', bbox_inches='tight', dpi=300)\n",
    "    fig.savefig(os.path.join(path, f\"{filename}.pdf\"), format='pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"Plot saved: {filename}\")\n",
    "\n",
    "def get_regression_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate all regression evaluation metrics.\"\"\"\n",
    "    metrics = {\n",
    "        'R-squared (R²)': r2_score(y_true, y_pred),\n",
    "        'Mean Absolute Error (MAE)': mean_absolute_error(y_true, y_pred),\n",
    "        'Mean Squared Error (MSE)': mean_squared_error(y_true, y_pred),\n",
    "        'Root Mean Squared Error (RMSE)': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, ax, title):\n",
    "    \"\"\"Plot actual vs. predicted values and annotate with regression stats.\"\"\"\n",
    "    ax.scatter(y_true, y_pred, alpha=0.5, edgecolors='k', s=20)\n",
    "    # The red dashed line represents the ideal scenario where Predicted = Actual\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2, label='Ideal Fit (y=x)')\n",
    "    ax.set_xlabel('Actual Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Calculate and add regression equation and R²\n",
    "    slope, intercept, r_value, _, _ = stats.linregress(y_true, y_pred)\n",
    "    r_squared = r_value**2\n",
    "    equation = f'y = {slope:.2f}x + {intercept:.2f}'\n",
    "    r2_text = f'R² = {r_squared:.3f}'\n",
    "    \n",
    "    ax.text(0.05, 0.95, f'{equation}\\n{r2_text}', transform=ax.transAxes, fontsize=7,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', fc='wheat', alpha=0.5))\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "\n",
    "def plot_residuals(y_true, y_pred, ax, title):\n",
    "    \"\"\"Plot residuals.\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    ax.scatter(y_pred, residuals, alpha=0.5, edgecolors='k', s=20)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "\n",
    "def plot_regression_seaborn(y_true, y_pred, ax, title):\n",
    "    \"\"\"Plot regression plot with seaborn, and annotate with R² and the regression equation.\"\"\"\n",
    "    sns.regplot(x=y_true, y=y_pred, ax=ax, scatter_kws={'alpha':0.3, 's':15}, line_kws={'color': 'red', 'lw': 2})\n",
    "    slope, intercept, r_value, _, _ = stats.linregress(y_true, y_pred)\n",
    "    r_squared = r_value**2\n",
    "    equation = f'y = {slope:.2f}x + {intercept:.2f}'\n",
    "    r2_text = f'R² = {r_squared:.3f}'\n",
    "    ax.text(0.05, 0.95, f'{equation}\\n{r2_text}', transform=ax.transAxes, fontsize=7,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', fc='wheat', alpha=0.5))\n",
    "    ax.set_xlabel('Actual Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"!!!!!!!!!! CRITICAL ERROR !!!!!!!!!!!\\nDataset file not found at:\\n{DATA_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 1: Exploratory Data Analysis (EDA)\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nPhase 1: Starting Exploratory Data Analysis (EDA)\")\n",
    "df.describe().to_excel(os.path.join(EDA_PATH, \"descriptive_statistics.xlsx\"))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(df[TARGET_VARIABLE], kde=True, ax=ax, color='teal')\n",
    "ax.set_title(f'Distribution of Target Variable: {TARGET_VARIABLE}')\n",
    "save_plot(fig, EDA_PATH, \"target_variable_distribution\", width_cm=12)\n",
    "\n",
    "numerical_features_all = df.select_dtypes(include=np.number).columns.tolist()\n",
    "corr_matrix = df[numerical_features_all].corr()\n",
    "fig, ax = plt.subplots(figsize=(20, 17))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".1f\", annot_kws={\"size\": 7})\n",
    "ax.set_title(\"Feature Correlation Matrix\")\n",
    "save_plot(fig, EDA_PATH, 'correlation_heatmap_all_features', width_cm=30)\n",
    "print(\"Exploratory Data Analysis completed.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 1.5: Inferential Statistical Analysis\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nPhase 1.5: Starting Inferential Statistical Analysis\")\n",
    "if 'label' in df.columns:\n",
    "    groups = [df[TARGET_VARIABLE][df['label'] == crop] for crop in df['label'].unique()]\n",
    "    groups = [g for g in groups if len(g) > 1]\n",
    "    if len(groups) > 1:\n",
    "        f_val, p_val = stats.f_oneway(*groups)\n",
    "        anova_results_df = pd.DataFrame([{'Comparison': f'Overall across all crops for {TARGET_VARIABLE}', 'F-statistic': f_val, 'p-value': p_val}])\n",
    "        anova_results_df.to_excel(os.path.join(STATS_PATH, 'anova_test_results.xlsx'), index=False)\n",
    "        print(\"Statistical analysis (ANOVA) completed and results saved.\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        sns.boxplot(x='label', y=TARGET_VARIABLE, data=df, ax=ax, palette=\"viridis\")\n",
    "        ax.set_title(f'Comparison of \"{TARGET_VARIABLE}\" by Crop Type')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        fig.tight_layout()\n",
    "        save_plot(fig, STATS_PATH, f'boxplot_{TARGET_VARIABLE}_by_crop', width_cm=20)\n",
    "    else:\n",
    "        print(\"Not enough groups found for ANOVA test.\")\n",
    "else:\n",
    "    print(\"Column 'label' not found in the dataset for ANOVA analysis.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 2: Data Preprocessing and Preparation\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nPhase 2: Starting data preprocessing\")\n",
    "if TARGET_VARIABLE not in df.columns:\n",
    "    raise ValueError(f\"Target variable '{TARGET_VARIABLE}' not found in the dataset!\")\n",
    "\n",
    "X = df.drop(TARGET_VARIABLE, axis=1)\n",
    "y = df[TARGET_VARIABLE]\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'label' not in categorical_features and 'label' in X.columns:\n",
    "    categorical_features.append('label')\n",
    "numerical_features = [f for f in numerical_features if f not in categorical_features]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "joblib.dump(preprocessor, os.path.join(DATA_RESULTS_PATH, 'preprocessor_regression.joblib'))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Saving train and test sets for reproducibility...\")\n",
    "X_train.to_csv(os.path.join(DATA_RESULTS_PATH, 'X_train.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(DATA_RESULTS_PATH, 'X_test.csv'), index=False)\n",
    "y_train.to_csv(os.path.join(DATA_RESULTS_PATH, 'y_train.csv'), index=False, header=True)\n",
    "y_test.to_csv(os.path.join(DATA_RESULTS_PATH, 'y_test.csv'), index=False, header=True)\n",
    "print(\"Data preprocessing and splitting completed. All necessary artifacts saved.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 3: Comprehensive Comparison of Regression Models\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nPhase 3: Starting comprehensive model evaluation and comparison\")\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(), 'Ridge': Ridge(random_state=RANDOM_STATE),\n",
    "    'Lasso': Lasso(random_state=RANDOM_STATE), 'KNN Regressor': KNeighborsRegressor(),\n",
    "    'SVR': SVR(), 'Random Forest Regressor': RandomForestRegressor(random_state=RANDOM_STATE),\n",
    "    'XGBoost Regressor': XGBRegressor(random_state=RANDOM_STATE),\n",
    "    'LightGBM Regressor': LGBMRegressor(random_state=RANDOM_STATE, verbosity=-1),\n",
    "}\n",
    "\n",
    "all_models_metrics = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Evaluating model: {name} ---\")\n",
    "    pipeline = Pipeline([('preprocessor', preprocessor), ('reg', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    model_filename = name.replace(' ', '_')\n",
    "    \n",
    "    predictions_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "    predictions_save_path = os.path.join(DATA_RESULTS_PATH, f\"predictions_{model_filename}.csv\")\n",
    "    predictions_df.to_csv(predictions_save_path, index=False)\n",
    "    print(f\"Predictions for {name} saved to: {predictions_save_path}\")\n",
    "\n",
    "    joblib.dump(pipeline, os.path.join(MODELS_PATH, f\"{model_filename}_pipeline.joblib\"))\n",
    "\n",
    "    metrics = get_regression_metrics(y_test, y_pred)\n",
    "    metrics['Model'] = name\n",
    "    all_models_metrics.append(metrics)\n",
    "    pd.DataFrame([metrics]).to_excel(os.path.join(MODELS_PATH, f'{model_filename}_metrics.xlsx'), index=False)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    fig.suptitle(f'Model Evaluation: {name}')\n",
    "    \n",
    "    plot_actual_vs_predicted(y_test, y_pred, ax1, 'Actual vs. Predicted')\n",
    "    plot_residuals(y_test, y_pred, ax2, 'Residuals Analysis')\n",
    "    plot_regression_seaborn(y_test, y_pred, ax3, 'Regression Plot (Seaborn)')\n",
    "    \n",
    "    # MODIFICATION 4: Increased wspace to create a wider gap between plots as requested.\n",
    "    # A wspace of 0.17 corresponds to an approximate 1.5 cm gap.\n",
    "    fig.subplots_adjust(left=0.05, right=0.98, wspace=0.17, top=0.9, bottom=0.1)\n",
    "    \n",
    "    save_plot(fig, MODELS_PATH, f'evaluation_plots_{model_filename}', width_cm=30)\n",
    "\n",
    "results_df = pd.DataFrame(all_models_metrics).set_index('Model').sort_values(by='R-squared (R²)', ascending=False)\n",
    "results_df.to_excel(os.path.join(COMPARE_PATH, \"all_models_metrics_comparison.xlsx\"))\n",
    "\n",
    "results_df_plot = results_df[['R-squared (R²)', 'Root Mean Squared Error (RMSE)']].reset_index()\n",
    "results_df_melted = results_df_plot.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "\n",
    "fig_metrics, ax_metrics = plt.subplots(figsize=(10, 7))\n",
    "sns.barplot(x='Score', y='Model', hue='Metric', data=results_df_melted, ax=ax_metrics, palette='viridis', orient='h')\n",
    "ax_metrics.set_title('Model Performance Comparison')\n",
    "ax_metrics.set_xlabel('Score')\n",
    "ax_metrics.set_ylabel('Model')\n",
    "fig_metrics.tight_layout()\n",
    "save_plot(fig_metrics, COMPARE_PATH, 'models_metrics_comparison_bar_chart', width_cm=20)\n",
    "\n",
    "best_model_name = results_df.index[0]\n",
    "print(f\"\\nBest model based on R-squared: {best_model_name}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 4: Hyperparameter Tuning of the Best Model\n",
    "# ------------------------------------------------------------------------------\n",
    "print(f\"\\nPhase 4: Starting hyperparameter tuning for {best_model_name}\")\n",
    "model_base = models[best_model_name]\n",
    "param_dist = {}\n",
    "\n",
    "if 'XGBoost' in best_model_name:\n",
    "    param_dist = {\n",
    "        'reg__n_estimators': [100, 300, 500, 700], 'reg__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'reg__max_depth': [3, 5, 7, 9], 'reg__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'reg__subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "    }\n",
    "elif 'Random Forest' in best_model_name:\n",
    "     param_dist = {\n",
    "        'reg__n_estimators': [100, 200, 300, 500], 'reg__max_depth': [10, 20, 30, None],\n",
    "        'reg__min_samples_split': [2, 5, 10], 'reg__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "elif 'LightGBM' in best_model_name:\n",
    "    param_dist = {\n",
    "        'reg__n_estimators': [100, 300, 500, 700], 'reg__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'reg__num_leaves': [20, 31, 40, 50], 'reg__max_depth': [-1, 5, 10],\n",
    "    }\n",
    "else:\n",
    "    print(\"Hyperparameter tuning is not defined for this model.\")\n",
    "    param_dist = None\n",
    "\n",
    "if param_dist:\n",
    "    pipeline = Pipeline([('preprocessor', preprocessor), ('reg', model_base)])\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline, param_distributions=param_dist, n_iter=50,\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "        scoring='neg_root_mean_squared_error', n_jobs=-1, random_state=RANDOM_STATE, verbose=1\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(\"\\nBest parameters found:\", random_search.best_params_)\n",
    "    final_model_pipeline = random_search.best_estimator_\n",
    "else:\n",
    "    final_model_pipeline = Pipeline([('preprocessor', preprocessor), ('reg', model_base)]).fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(final_model_pipeline, os.path.join(BEST_MODEL_PATH, \"final_tuned_model_pipeline.joblib\"))\n",
    "print(f\"Tuned model {best_model_name} saved in the Best_Model_Analysis folder.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 5: In-depth Analysis of the Best Model with SHAP\n",
    "# ------------------------------------------------------------------------------\n",
    "print(f\"\\nPhase 5: Starting in-depth analysis of {best_model_name} with SHAP\")\n",
    "\n",
    "final_model = final_model_pipeline.named_steps['reg']\n",
    "preprocessor_fitted = final_model_pipeline.named_steps['preprocessor']\n",
    "X_test_transformed = preprocessor_fitted.transform(X_test)\n",
    "feature_names_out = preprocessor_fitted.get_feature_names_out()\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=feature_names_out, index=X_test.index)\n",
    "\n",
    "try:\n",
    "    if isinstance(final_model, (RandomForestRegressor, XGBRegressor, LGBMRegressor)):\n",
    "        explainer = shap.TreeExplainer(final_model)\n",
    "    else:\n",
    "        X_train_transformed_summary = shap.sample(preprocessor_fitted.transform(X_train), 100)\n",
    "        explainer = shap.KernelExplainer(final_model.predict, X_train_transformed_summary)\n",
    "\n",
    "    shap_values = explainer(X_test_transformed_df)\n",
    "    \n",
    "    shap_data_to_save = {\n",
    "        'shap_values': shap_values.values,\n",
    "        'base_values': shap_values.base_values,\n",
    "        'data': X_test_transformed_df\n",
    "    }\n",
    "    joblib.dump(shap_data_to_save, os.path.join(DATA_RESULTS_PATH, 'shap_data.joblib'))\n",
    "    print(\"SHAP values and transformed test data saved for reproducibility.\")\n",
    "\n",
    "    shap.summary_plot(shap_values, X_test_transformed_df, plot_type=\"bar\", show=False, max_display=20)\n",
    "    fig_shap_bar = plt.gcf()\n",
    "    plt.title(f\"SHAP Feature Importance (Model: {best_model_name})\")\n",
    "    save_plot(fig_shap_bar, BEST_MODEL_PATH, \"shap_summary_bar_plot\", width_cm=20)\n",
    "    \n",
    "    shap.summary_plot(shap_values, X_test_transformed_df, show=False, max_display=20)\n",
    "    fig_shap_dot = plt.gcf()\n",
    "    plt.title(f\"SHAP Summary of Feature Effects (Model: {best_model_name})\")\n",
    "    save_plot(fig_shap_dot, BEST_MODEL_PATH, \"shap_summary_dot_plot\", width_cm=20)\n",
    "    \n",
    "    top_features = X_test_transformed_df.columns[np.argsort(np.abs(shap_values.values).mean(0))][::-1]\n",
    "    \n",
    "    print(\"Creating SHAP dependence plots for top features...\")\n",
    "    for feature in top_features[:5]:\n",
    "        fig_dep, ax_dep = plt.subplots()\n",
    "        shap.dependence_plot(feature, shap_values.values, X_test_transformed_df, ax=ax_dep, show=False)\n",
    "        ax_dep.set_title(f'SHAP Dependence Plot for Feature: {feature}')\n",
    "        save_plot(fig_dep, BEST_MODEL_PATH, f\"shap_dependence_plot_{feature.replace('/', '_').replace('<', '_lt_')}\", width_cm=15)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"SHAP analysis failed with error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\nMain script executed successfully! All results and artifacts are saved.\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting Results Reproduction Script\n",
      "================================================================================\n",
      "\n",
      "Phase 0: Setting up paths and directories\n",
      "Reproduction directories created successfully.\n",
      "Original dataset loaded for EDA and Stats reproduction.\n",
      "\n",
      "Phase 1: Reproducing Exploratory Data Analysis (EDA)\n",
      "Table re-generated: descriptive_statistics.xlsx\n",
      "Plot re-generated: target_variable_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot re-generated: correlation_heatmap_all_features\n",
      "EDA reproduction completed.\n",
      "\n",
      "Phase 2: Reproducing Inferential Statistical Analysis\n",
      "Table re-generated: anova_test_results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3914310/2102214863.py:148: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(x='label', y=TARGET_VARIABLE, data=df, ax=ax, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot re-generated: boxplot_water_usage_efficiency_by_crop\n",
      "Statistical Analysis reproduction completed.\n",
      "\n",
      "Phase 3 & 4: Reproducing Model Evaluations and Comparison\n",
      "--- Reproducing results for model: Linear Regression ---\n",
      "  -> Metrics re-generated for Linear Regression.\n",
      "Plot re-generated: evaluation_plots_Linear_Regression\n",
      "--- Reproducing results for model: Ridge ---\n",
      "  -> Metrics re-generated for Ridge.\n",
      "Plot re-generated: evaluation_plots_Ridge\n",
      "--- Reproducing results for model: Lasso ---\n",
      "  -> Metrics re-generated for Lasso.\n",
      "Plot re-generated: evaluation_plots_Lasso\n",
      "--- Reproducing results for model: KNN Regressor ---\n",
      "  -> Metrics re-generated for KNN Regressor.\n",
      "Plot re-generated: evaluation_plots_KNN_Regressor\n",
      "--- Reproducing results for model: SVR ---\n",
      "  -> Metrics re-generated for SVR.\n",
      "Plot re-generated: evaluation_plots_SVR\n",
      "--- Reproducing results for model: Random Forest Regressor ---\n",
      "  -> Metrics re-generated for Random Forest Regressor.\n",
      "Plot re-generated: evaluation_plots_Random_Forest_Regressor\n",
      "--- Reproducing results for model: XGBoost Regressor ---\n",
      "  -> Metrics re-generated for XGBoost Regressor.\n",
      "Plot re-generated: evaluation_plots_XGBoost_Regressor\n",
      "--- Reproducing results for model: LightGBM Regressor ---\n",
      "  -> Metrics re-generated for LightGBM Regressor.\n",
      "Plot re-generated: evaluation_plots_LightGBM_Regressor\n",
      "Table re-generated: all_models_metrics_comparison.xlsx\n",
      "Plot re-generated: models_metrics_comparison_bar_chart\n",
      "Best model identified from reproduced metrics: Lasso\n",
      "Model Evaluations and Comparison reproduction completed.\n",
      "\n",
      "Phase 5: Reproducing SHAP analysis for the best model\n",
      "SHAP data loaded successfully. Re-generating plots...\n",
      "Plot re-generated: shap_summary_bar_plot\n",
      "Plot re-generated: shap_summary_dot_plot\n",
      "Re-creating SHAP dependence plots for top features...\n",
      "Plot re-generated: shap_dependence_plot_cat__label_watermelon\n",
      "Plot re-generated: shap_dependence_plot_cat__label_rice\n",
      "Plot re-generated: shap_dependence_plot_cat__label_pomegranate\n",
      "Plot re-generated: shap_dependence_plot_cat__label_pigeonpeas\n",
      "Plot re-generated: shap_dependence_plot_cat__label_papaya\n",
      "\n",
      "================================================================================\n",
      "Reproduction script finished! All results are re-generated in the 'reproduced_results' folder.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
